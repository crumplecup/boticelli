//! Data transfer objects for OpenAI-compatible APIs.

use derive_builder::Builder;
use derive_getters::Getters;
use serde::{Deserialize, Serialize};

/// A message in the OpenAI chat format.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// Role: "system", "user", or "assistant"
    pub role: String,
    /// Message content
    pub content: String,
}

/// OpenAI chat completion request.
#[derive(Debug, Clone, Serialize, Builder, Getters)]
#[builder(setter(into))]
pub struct ChatRequest {
    /// Model identifier
    model: String,
    /// Conversation messages
    messages: Vec<ChatMessage>,
    /// Maximum tokens to generate
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    /// Sampling temperature
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    /// Enable streaming
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,
}

impl ChatRequest {
    /// Creates a new builder for ChatRequest.
    pub fn builder() -> ChatRequestBuilder {
        ChatRequestBuilder::default()
    }
}

/// A choice in the OpenAI response.
///
/// Represents one possible completion from the model. In non-streaming mode,
/// typically contains a single choice with the complete response.
#[derive(Debug, Clone, Deserialize, derive_getters::Getters)]
pub struct ChatChoice {
    /// The message content returned by the model
    pub message: ChatMessage,

    /// Reason the model stopped generating
    ///
    /// Common values: "stop" (natural completion), "length" (max tokens reached),
    /// "content_filter" (filtered by safety systems).
    /// Deserialized from API response.
    #[serde(default)]
    pub finish_reason: Option<String>,
}

/// Token usage statistics for a completion request.
///
/// Tracks token consumption for billing and rate limiting purposes.
/// All providers return slightly different formats, so fields are optional.
/// All fields are public as this is a DTO deserialized from API responses.
#[derive(Debug, Clone, Deserialize, derive_getters::Getters)]
pub struct ChatUsage {
    /// Number of tokens in the input prompt
    ///
    /// Used to calculate input costs and track rate limits.
    /// Deserialized from API response.
    #[serde(default)]
    pub prompt_tokens: Option<usize>,

    /// Number of tokens in the generated completion
    ///
    /// Used to calculate output costs (typically higher than input).
    /// Deserialized from API response.
    #[serde(default)]
    pub completion_tokens: Option<usize>,

    /// Total tokens used (prompt + completion)
    ///
    /// May differ slightly from sum due to provider-specific counting.
    /// Deserialized from API response.
    #[serde(default)]
    pub total_tokens: Option<usize>,
}

/// OpenAI chat completion response.
///
/// Returned by OpenAI-compatible APIs after a successful completion request.
/// Contains the generated text and metadata about token usage.
/// All fields are public as this is a DTO deserialized from API responses.
#[derive(Debug, Clone, Deserialize, derive_getters::Getters)]
pub struct ChatResponse {
    /// One or more completion choices
    ///
    /// Non-streaming requests typically return a single choice.
    /// The `n` parameter in the request controls how many choices are returned.
    pub choices: Vec<ChatChoice>,

    /// Token usage statistics for this request
    ///
    /// Used for billing calculations and rate limit tracking.
    /// May be absent in streaming responses or error cases.
    /// Deserialized from API response.
    #[serde(default)]
    pub usage: Option<ChatUsage>,
}

/// Errors from OpenAI-compatible APIs.
#[derive(Debug, Clone, derive_more::Display)]
pub enum OpenAICompatError {
    /// HTTP/network error
    #[display("HTTP error: {}", _0)]
    Http(String),

    /// API returned an error
    #[display("API error (status {}): {}", status, message)]
    Api {
        /// HTTP status code
        status: u16,
        /// Error message
        message: String,
    },

    /// Rate limit exceeded
    #[display("Rate limit exceeded")]
    RateLimit,

    /// Model not found
    #[display("Model not found: {}", _0)]
    ModelNotFound(String),

    /// Invalid request
    #[display("Invalid request: {}", _0)]
    InvalidRequest(String),

    /// Failed to parse response
    #[display("Response parsing failed: {}", _0)]
    ResponseParsing(String),

    /// Builder error
    #[display("Builder error: {}", _0)]
    Builder(String),
}

impl std::error::Error for OpenAICompatError {}
